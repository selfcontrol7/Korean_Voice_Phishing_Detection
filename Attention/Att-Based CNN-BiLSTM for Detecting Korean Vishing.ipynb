{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "retired-stomach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T13:18:36.623530900Z",
     "start_time": "2024-01-22T13:18:35.476876500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 22 22:18:36 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 545.36                 Driver Version: 546.33       CUDA Version: 12.3     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    On  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| N/A   44C    P8               2W /  90W |    153MiB /  8188MiB |     17%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297a6371064289d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T13:18:44.935712200Z",
     "start_time": "2024-01-22T13:18:36.623530900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-22 22:18:39.400721: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n",
      "2024-01-22 22:18:39.628332: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-01-22 22:18:39.628459: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-01-22 22:18:39.647036: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2024-01-22 22:18:39.723610: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-01-22 22:18:42.408887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "2024-01-22 22:18:44.171442: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2024-01-22 22:18:44.481164: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "2024-01-22 22:18:44.481270: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\r\n",
      "Your kernel may have been built without NUMA support.\r\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-james",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worth-transmission",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T09:52:04.417418Z",
     "start_time": "2024-01-29T09:52:00.397592100Z"
    },
    "executionInfo": {
     "elapsed": 2360,
     "status": "ok",
     "timestamp": 1650012512493,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "wJWGvzLaj3Y7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 18:52:01.051800: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-29 18:52:01.220143: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-29 18:52:01.220267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-29 18:52:01.237101: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-29 18:52:01.305912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-29 18:52:02.975415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputSpec, Layer, Input, Embedding, Conv1D, Conv2D, Bidirectional, Dense, Attention\n",
    "from tensorflow.keras.layers import LSTM, Activation, Add, Flatten, Concatenate, concatenate # CuDNNGRU, CuDNNLSTM, \n",
    "from tensorflow.keras.layers import Reshape, Dropout, SpatialDropout1D, BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling1D, MaxPool2D, GlobalAveragePooling1D, GlobalMaxPooling1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import os, re, csv, math, codecs, copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import os, re, csv, math, codecs\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-jacksonville",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ploting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "electronic-stopping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T13:18:46.520653700Z",
     "start_time": "2024-01-22T13:18:46.515474100Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "    '''\n",
    "    Heatmap with text in each cell with matplotlib's pyplot\n",
    "    Source: https://stackoverflow.com/a/25074150/395857 \n",
    "    By HYRY\n",
    "    '''\n",
    "#     from itertools import izip\n",
    "    pc.update_scalarmappable()\n",
    "    ax = pc.axes# FOR LATEST MATPLOTLIB\n",
    "    \n",
    "    #Use zip BELOW IN PYTHON 3\n",
    "    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.all(color[:3] > 0.5):\n",
    "            color = (0.0, 0.0, 0.0)\n",
    "        else:\n",
    "            color = (1.0, 1.0, 1.0)\n",
    "        ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "\n",
    "\n",
    "def cm2inch(*tupl):\n",
    "    '''\n",
    "    Specify figure size in centimeter in matplotlib\n",
    "    Source: https://stackoverflow.com/a/22787457/395857\n",
    "    By gns-ank\n",
    "    '''\n",
    "    inch = 2.54\n",
    "    if type(tupl[0]) == tuple:\n",
    "        return tuple(i/inch for i in tupl[0])\n",
    "    else:\n",
    "        return tuple(i/inch for i in tupl)\n",
    "\n",
    "\n",
    "def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):\n",
    "    '''\n",
    "    Inspired by:\n",
    "    - https://stackoverflow.com/a/16124677/395857 \n",
    "    - https://stackoverflow.com/a/25074150/395857\n",
    "    '''\n",
    "\n",
    "    # Plot it out\n",
    "    fig, ax = plt.subplots()    \n",
    "    #c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap='RdBu', vmin=0.0, vmax=1.0)\n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    "\n",
    "    # set tick labels\n",
    "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
    "    ax.set_xticklabels(xticklabels, minor=False)\n",
    "    ax.set_yticklabels(yticklabels, minor=False)\n",
    "\n",
    "    # set title and x/y labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)      \n",
    "\n",
    "    # Remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()    \n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "\n",
    "    # Add color bar\n",
    "    plt.colorbar(c)\n",
    "\n",
    "    # Add text in each cell \n",
    "    show_values(c)\n",
    "\n",
    "    # Proper orientation (origin at the top left instead of bottom left)\n",
    "    if correct_orientation:\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()       \n",
    "\n",
    "    # resize \n",
    "    fig = plt.gcf()\n",
    "    #fig.set_size_inches(cm2inch(40, 20))\n",
    "    #fig.set_size_inches(cm2inch(40*4, 20*4))\n",
    "    fig.set_size_inches(cm2inch(figure_width, figure_height))\n",
    "\n",
    "\n",
    "\n",
    "def plot_classification_report(classification_report, title='Classification report ', cmap='RdBu'):\n",
    "    '''\n",
    "    Plot scikit-learn classification report.\n",
    "    Extension based on https://stackoverflow.com/a/31689645/395857 \n",
    "    '''\n",
    "    lines = classification_report.split('\\n')\n",
    "\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    support = []\n",
    "    class_names = []\n",
    "    for line in lines[2 : (len(lines) - 4)]:\n",
    "        t = line.strip().split()\n",
    "        if len(t) < 2: continue\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        support.append(int(t[-1]))\n",
    "        class_names.append(t[0])\n",
    "        print(v)\n",
    "        plotMat.append(v)\n",
    "\n",
    "    print('plotMat: {0}'.format(plotMat))\n",
    "    print('support: {0}'.format(support))\n",
    "\n",
    "    xlabel = 'Metrics'\n",
    "    ylabel = 'Classes'\n",
    "    xticklabels = ['Precision', 'Recall', 'F1-score']\n",
    "    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n",
    "    figure_width = 25\n",
    "    figure_height = len(class_names) + 7\n",
    "    correct_orientation = False\n",
    "    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-electronics",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import the NPZ file of our dataset\n",
    "#### This file was created using the code from the file Korean_Voice_Phishing_Detection/ML_DL_models/DL/data_npz_creation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wireless-innocent",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T13:18:58.917019400Z",
     "start_time": "2024-01-22T13:18:46.517473500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3625,
     "status": "ok",
     "timestamp": 1650012516114,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "a49GcNcPqQTO",
    "outputId": "ec1e9665-d342-46e3-ab8e-da00ae12d310",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 879 ms, sys: 472 ms, total: 1.35 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "npsavezFile=np.load(\"outfile_space_20220426.npz\",allow_pickle=True)\n",
    "# print(npsavezFile.files)\n",
    "\n",
    "vocab=npsavezFile['arr_1']\n",
    "data=npsavezFile['arr_0']\n",
    "Y=npsavezFile['arr_2']\n",
    "Y=list(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mexican-lindsay",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T13:18:58.929018400Z",
     "start_time": "2024-01-22T13:18:58.916009Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650012516115,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "_NqUBABnfyWc",
    "outputId": "1a3b76a1-4b7a-49aa-8c16-729d233956ca",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['가가지고', '가가지고다시', '가거나', ..., '힙합하는', '힙합하면은', '힝'], dtype='<U24')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "changing-puppy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T13:18:58.933544800Z",
     "start_time": "2024-01-22T13:18:58.919008100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650012516117,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "XZm6au5yqUDo",
    "outputId": "6ad9b74f-413a-4ecc-83b3-bcc1fed8f864",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 110 µs, sys: 0 ns, total: 110 µs\n",
      "Wall time: 112 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_length=max([len(i) for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "paperback-airfare",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T13:18:59.076161Z",
     "start_time": "2024-01-22T13:18:58.924009400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1650012516916,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "b3WO0mQ1qZt1",
    "outputId": "22387c1d-ebff-4ebc-a312-54cafb919655",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 142 ms, sys: 0 ns, total: 142 ms\n",
      "Wall time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_id=dict((c,i) for i,c in enumerate(vocab))\n",
    "id_word=dict((i,c) for i,c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "committed-electricity",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1650012517394,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "v7YygBggqdnJ",
    "outputId": "320d859e-0153-4352-ce8d-7b4b5840a39a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train=[]\n",
    "for sentence in data:\n",
    "    x=[]\n",
    "    for word in sentence:\n",
    "        app=word_id[word]\n",
    "        x.append(app)\n",
    "        \n",
    "    X_train.append(x)\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ac900d179bbb0a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T13:36:39.501075500Z",
     "start_time": "2024-01-22T13:36:39.384992800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['가가지고', '가가지고다시', '가거나', ..., '힙합하는', '힙합하면은', '힝'], dtype='<U24')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25053e432e417c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T13:36:10.917324500Z",
     "start_time": "2024-01-22T13:36:10.854244400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2924"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-shakespeare",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1650012517867,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "NzkA41ejqhCD",
    "outputId": "4fb641a1-4966-4e75-d034-9158f34e707c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "max_length=max([len(i) for i in X_train])\n",
    "\n",
    "encoded = pad_sequences(X_train, maxlen=max_length, padding='pre')\n",
    "encoded=np.array(encoded)\n",
    "print(\"encoded shape\",encoded.shape) #encoded shape (12924, 35)\n",
    "print(\"max_length\", max_length)\n",
    "\n",
    "length_encoded=len(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-cloud",
   "metadata": {
    "id": "vUsGUNUhiUkj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import the Korean fastText pre-trained model and perform the embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "textile-strand",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T09:52:15.082690800Z",
     "start_time": "2024-01-29T09:52:15.023171500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1650012517868,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "eNoDFqlw4789",
    "outputId": "9b411984-98ac-42ab-9491-9d53e4bc987a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki.ko.vec exists, will not download file from the internet\n",
      "CPU times: user 965 µs, sys: 500 µs, total: 1.47 ms\n",
      "Wall time: 8.42 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run if the wiki.ko.vec is not available in the same directory\n",
    "import os\n",
    "import urllib.request\n",
    "# check if the file wiki.ko.vec is in the directory if not download it\n",
    "if not os.path.isfile('wiki.ko.vec'):\n",
    "    print('wiki.ko.vec does not exist, downloading file from the internet')\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.ko.vec', 'wiki.ko.vec')\n",
    "else:\n",
    "    print('wiki.ko.vec exists, will not download file from the internet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wrong-memorabilia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T10:01:32.098830300Z",
     "start_time": "2024-01-29T09:52:15.746810300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94564,
     "status": "ok",
     "timestamp": 1650012612426,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "buOoByf6qklN",
    "outputId": "3f2bbb00-92e3-4442-a1fa-06bbf2f77120",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word FastText embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "879131it [09:16, 1580.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 23 s, total: 1min 40s\n",
      "Wall time: 9min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('loading word FastText embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('wiki.ko.vec', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "concerned-berry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T10:01:48.606659700Z",
     "start_time": "2024-01-29T10:01:48.536124Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4855,
     "status": "ok",
     "timestamp": 1650012617251,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "Gt7pm3GdqozW",
    "outputId": "ff21f3a0-0ae9-4f66-d595-b74847d22a21",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 879130 word vectors\n",
      "tokenizing input data...\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "MAX_NB_WORDS=len(vocab)\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "print(\"tokenizing input data...\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=False, char_level=False)\n",
    "tokenizer.fit_on_texts(data)  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(data)\n",
    "word_seq_test = tokenizer.texts_to_sequences(data)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "relevant-recipe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T07:36:34.708572500Z",
     "start_time": "2024-01-30T07:36:34.537064800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1650012617861,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "moIwO9vvqsLF",
    "outputId": "7585d6c6-9a27-45dc-8699-8d44d113bfb7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MAX_NB_WORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_NB_WORDS' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "embed_dim = 300 # 32 Dimensions of the embedding vector\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((MAX_NB_WORDS, embed_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-mining",
   "metadata": {
    "id": "ch-jfKMGimP3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Split the dataset/embedding matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-mortality",
   "metadata": {
    "id": "UD5l3WtUB1wF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Y_train=np.array(Y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded, Y_train, test_size=0.1, random_state=42, shuffle= True)\n",
    "\n",
    "# Vectorize the output sentence type classifications to Keras readable format\n",
    "y_train=to_categorical(y_train, num_classes=2)\n",
    "y_test=to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-marketing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1650014427972,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "zg9JG59WSWHy",
    "outputId": "92b7957b-6837-4e1a-9cea-2a4d332db8a1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the shape of each set\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-adapter",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650014427973,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "a608XJ7QChfq",
    "outputId": "70d5df45-c7f9-4319-c081-6cd8774e9bb4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Train set size = {} \\nTest set size = {}'.format(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-charlotte",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build Text 2D CNN Model\n",
    "##### https://www.kaggle.com/mlwhiz/textcnn-pytorch-and-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-nigeria",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model_train_cv(x_train,y_train,nfold,model_obj):\n",
    "    splits = list(StratifiedKFold(n_splits=nfold, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "    x_train = x_train\n",
    "    y_train = np.array(y_train)\n",
    "    # matrix for the out-of-fold predictions\n",
    "    train_oof_preds = np.zeros((x_train.shape[0]))\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f'Fold {i + 1}')\n",
    "        x_train_fold = x_train[train_idx.astype(int)]\n",
    "        y_train_fold = y_train[train_idx.astype(int)]\n",
    "        x_val_fold = x_train[valid_idx.astype(int)]\n",
    "        y_val_fold = y_train[valid_idx.astype(int)]\n",
    "\n",
    "        clf = copy.deepcopy(model_obj)\n",
    "        clf.fit(x_train_fold, y_train_fold, batch_size=512, epochs=5, validation_data=(x_val_fold, y_val_fold))\n",
    "        \n",
    "        valid_preds_fold = clf.predict(x_val_fold)[:,0]\n",
    "\n",
    "        # storing OOF predictions\n",
    "        train_oof_preds[valid_idx] = valid_preds_fold\n",
    "    return train_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-hundred",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bestThresshold(y_train,train_preds):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "    return delta , tmp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-galaxy",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model_cnn(embedding_matrix):\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 36\n",
    "\n",
    "    inp = Input(shape=(max_length,))\n",
    "    x = Embedding(MAX_NB_WORDS, embed_dim, weights=[embedding_matrix])(inp)\n",
    "    x = Reshape((max_length, embed_dim, 1))(x)\n",
    "\n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_dim),\n",
    "                                     kernel_initializer='he_normal', activation='relu')(x)\n",
    "        maxpool_pool.append(MaxPool2D(pool_size=(max_length - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "    z = Concatenate(axis=1)(maxpool_pool)   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "\n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Display a summary of the models structure\n",
    "    print(model.summary())\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-england",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build BiLSTM\n",
    "##### https://www.kaggle.com/code/mlwhiz/bilstm-pytorch-and-keras/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-jewelry",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model_train_cv(x_train,y_train,nfold,model_obj):\n",
    "    splits = list(StratifiedKFold(n_splits=nfold, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "    x_train = x_train\n",
    "    y_train = np.array(y_train)\n",
    "    # matrix for the out-of-fold predictions\n",
    "    train_oof_preds = np.zeros((x_train.shape[0]))\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f'Fold {i + 1}')\n",
    "        x_train_fold = x_train[train_idx.astype(int)]\n",
    "        y_train_fold = y_train[train_idx.astype(int)]\n",
    "        x_val_fold = x_train[valid_idx.astype(int)]\n",
    "        y_val_fold = y_train[valid_idx.astype(int)]\n",
    "\n",
    "        clf = copy.deepcopy(model_obj)\n",
    "        clf.fit(x_train_fold, y_train_fold, batch_size=512, epochs=5, validation_data=(x_val_fold, y_val_fold))\n",
    "        \n",
    "        valid_preds_fold = clf.predict(x_val_fold)[:,0]\n",
    "\n",
    "        # storing OOF predictions\n",
    "        train_oof_preds[valid_idx] = valid_preds_fold\n",
    "    return train_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-uniform",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BiDirectional LSTM\n",
    "\n",
    "def model_lstm_du(embedding_matrix):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    x = Embedding(MAX_NB_WORDS, embed_dim, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    '''\n",
    "    Here 64 is the size(dim) of the hidden state vector as well as the output vector. \n",
    "    Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?\n",
    "        64*70(maxlen)*2(bidirection concat)\n",
    "    CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU\n",
    "    '''\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    conc = Dense(64, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    \n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Display a summary of the models structure\n",
    "    print(model.summary())\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-sunglasses",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build the Attention-Based CNN-BiLSTM Model\n",
    "## That model is a model that performs a binary classification problem\n",
    "### https://wikidocs.net/80783\n",
    "### https://wikidocs.net/85337\n",
    "### https://www.kaggle.com/code/duykhanh99/bidirectional-lstm-cnn-attention-model/notebook\n",
    "### https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-cornell",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bahdanau Attention\n",
    "##### https://wikidocs.net/48920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-senator",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = Dense(units)\n",
    "    self.W2 = Dense(units)\n",
    "    self.V = Dense(1)\n",
    "\n",
    "  def call(self, values, query): # 단, key와 value는 같음\n",
    "    # query shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-ribbon",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems\n",
    "###### https://arxiv.org/abs/1512.08756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-citizenship",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Keras Layer that implements an Attention mechanism for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    :param kwargs:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(Attention())\n",
    "    \"\"\"\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-mountain",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hierarchical Attention Networks for Document Classification\n",
    "##### http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-meditation",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566\n",
    "# https://www.kaggle.com/code/duykhanh99/bidirectional-lstm-cnn-attention-model/notebook\n",
    "    \n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "    \n",
    "def create_custom_objects():\n",
    "    instance_holder = {\"instance\": None}\n",
    "\n",
    "    class ClassWrapper(AttentionWithContext):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            instance_holder[\"instance\"] = self\n",
    "            super(ClassWrapper, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def loss(*args):\n",
    "        method = getattr(instance_holder[\"instance\"], \"loss_function\")\n",
    "        return method(*args)\n",
    "\n",
    "    def accuracy(*args):\n",
    "        method = getattr(instance_holder[\"instance\"], \"accuracy\")\n",
    "        return method(*args)\n",
    "    return {\"ClassWrapper\": ClassWrapper ,\"AttentionWithContext\": ClassWrapper, \"loss\": loss,\n",
    "            \"accuracy\":accuracy}\n",
    "\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "        \n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#             \"arg1\": self.arg1,\n",
    "#             \"arg2\": self.arg2,\n",
    "#         })\n",
    "#         return config     \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "                'W_regularizer': self.W_regularizer,\n",
    "                'u_regularizer': self.u_regularizer,\n",
    "                'b_regularizer': self.b_regularizer,\n",
    "                'W_constraint': self.W_constraint,\n",
    "                'u_constraint': self.u_constraint,\n",
    "                'b_constraint': self.b_constraint,\n",
    "                'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-forth",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-wright",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up our results dataframe\n",
    "df_results = pd.DataFrame(columns=['F1_score', 'Precision', 'Recall', 'Accuracy', 'Training time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-implement",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# defining the hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size= 32 # 64, 128\n",
    "epochs = 10\n",
    "steps_per_epoch = len(X_train) // batch_size  # total_samples is the training set size\n",
    "\n",
    "# Calculating decay steps\n",
    "# It's common to decay the learning rate at each epoch\n",
    "decay_steps = steps_per_epoch * epochs # 10000\n",
    "decay_rate = 0.9  # This is a common decay rate, but you can adjust it\n",
    "# learning_decay = 1e-10 # 1e-4\n",
    "\n",
    "spa_dropout_ratio = 0.2 # dropout ration, dropping a entire feature map\n",
    "kernel_size = 3 # [1,2,3,5] # [1,2,3,5] Size of the kernel. Mixing kernels of various sizes.\n",
    "                # specifying the length of the 1D convolution window.\n",
    "dense_units = 64 # hidden unit 128 the number of neurons in the hidden layer\n",
    "dropout_ratio = 0.2 # 0.1, 0.2 to 0.5 Dropout Ratio\n",
    "num_filters = 50 # 36, 128, 256 number of kernels, conv_size\n",
    "                # Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "\n",
    "lstm_units_1 = 64 # the size(dim) of the hidden state vector as well as the output vector.\n",
    "lstm_units_2 = 32 # the size(dim) of the hidden state vector as well as the output vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-improvement",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Att CNN-BiLSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-scheme",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_1(lr=0.0, lr_d=0.0, spatial_dr=0.0, kernel_size=3,\n",
    "                  dense_units=128, dropout_ratio=0.0, num_filters=32,\n",
    "                  lstm_units_1=0, lstm_units_2=0, batch_size=64, epochs=10):\n",
    "    \n",
    "    model_input = Input(shape=(max_length,))\n",
    "    \n",
    "    ######################## EMBEDDING LAYER ###############################################\n",
    "    # embed MAX_NB_WORDS words into a embed_dim vector. (e.g. For fasttext, the embed__dim is 300)\n",
    "    x = Embedding(\n",
    "        MAX_NB_WORDS, \n",
    "        embed_dim, \n",
    "        input_length = max_length,\n",
    "        weights = [embedding_matrix], \n",
    "        trainable = False,\n",
    "        name = \"embedding\"\n",
    "        )(model_input)\n",
    "        \n",
    "    x = SpatialDropout1D(spatial_dr)(x)\n",
    "    \n",
    "    ######################## CNN LAYER(S) ###############################################\n",
    "    conv = Conv1D(filters=num_filters, kernel_size=kernel_size, padding=\"valid\", activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(conv)    \n",
    "    x = Dropout(dropout_ratio)(x) # yes or no\n",
    "   \n",
    "    ######################## BiLSTM LAYER(S) ################################################\n",
    "    # CuDNNLSTm can be implemented instead of LSTM, it is just implementations of LSTM that are created to run faster on GPUs\n",
    "    x = Bidirectional(LSTM(lstm_units_1, return_sequences=True, kernel_initializer=initializers.glorot_uniform(seed=0)))(x)\n",
    "    x = Bidirectional(LSTM(lstm_units_2, return_sequences=True, kernel_initializer=initializers.glorot_uniform(seed=0)))(x) # yes or no\n",
    "    \n",
    "    ######################## ATTENTION LAYER ################################################\n",
    "    # use Attention operation, with a context/query vector, for temporal data.\n",
    "    x = AttentionWithContext()(x)\n",
    "    x = Dense(dense_units, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=0))(x)  \n",
    "    x = Dropout(dropout_ratio)(x)\n",
    "\n",
    "    # The attention using Bahdanau attention.\n",
    "#     attention = BahdanauAttention(lstm_units_2) # Weight Size Definition\n",
    "#     context_vector, attention_weights = attention(lstm, state_h)\n",
    "\n",
    "    # use Attention mechanism for temporal data.\n",
    "#     x = Attention(max_length)(x)\n",
    "#     x = GlobalMaxPooling1D()(x) # yes or no    \n",
    "#     x = GlobalAveragePooling1D()(x) # yes or no    \n",
    "#     x = Dropout(dropout_ratio)(x)\n",
    "#     x = Dense(64, activation='relu')(x)\n",
    "#     x = Dropout(dropout_ratio)(x)\n",
    "\n",
    "    ######################## CLASSIFICATION LAYER ###########################################\n",
    "    model_output = Dense(2, activation=\"softmax\", kernel_initializer=initializers.glorot_uniform(seed=0))(x)\n",
    "    model = Model(model_input, model_output)\n",
    "    \n",
    "    ######################## COMPILING THE MODEL ##################################\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss = \"categorical_crossentropy\", \n",
    "        # optimizer = Adam(learning_rate=lr, decay=lr_d),\n",
    "        optimizer = Adam(learning_rate=lr_schedule),\n",
    "        metrics = [\n",
    "            \"accuracy\",\n",
    "            \"binary_accuracy\",\n",
    "            # 'categorical_accuracy',\n",
    "            # tf.keras.metrics.AUC(),\n",
    "            # tf.keras.metrics.Precision(),\n",
    "            # tf.keras.metrics.Recall()\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    ######################## MODEL SAVING/OVERFITTING MANAGEMENT ##################################\n",
    "    model_path = \"models/best_Attention_CNN_BiLSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + \".hdf5\"\n",
    "    check_point = ModelCheckpoint(model_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3, verbose=1,)\n",
    "    \n",
    "    # Display a summary of the models structure\n",
    "    print(model.summary())\n",
    "    print(\"#\"*80)\n",
    "    tf.keras.utils.plot_model(model, to_file=\"reports/Att_cnn_BiLSTM_architecture_\"+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + \".png\", show_shapes=True)\n",
    "    \n",
    "    ######################## TRAINING THE MODEL ##################################\n",
    "    start_time = time()\n",
    "    history_1 = model.fit(\n",
    "        X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "        validation_split=0.11111111111111111111111111111,\n",
    "        callbacks=[check_point,early_stop]\n",
    "    )\n",
    "    train_time = time() - start_time\n",
    "    print(train_time)\n",
    "\n",
    "#     model = load_model(model_path)\n",
    "    model = load_model(model_path, custom_objects=create_custom_objects())\n",
    "    \n",
    "    ######################## EVALUATING THE MODEL ##################################  \n",
    "    model.evaluate(X_test, y_test) # or model.predict\n",
    "    \n",
    "    return model, history_1, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-score",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train proposed model\n",
    "model_1, history_1, train_time = build_model_1(\n",
    "    lr=learning_rate, \n",
    "    # lr_d=learning_decay,\n",
    "    spatial_dr=spa_dropout_ratio, \n",
    "    kernel_size=kernel_size, \n",
    "    dense_units=dense_units, \n",
    "    dropout_ratio=dropout_ratio, \n",
    "    num_filters=num_filters, \n",
    "    lstm_units_1=lstm_units_1,\n",
    "    lstm_units_2=lstm_units_2,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18532b2fa0f81877",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b99bf61d0e95bc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_1.epoch, history_1.history['accuracy'], '-o', label='Training_accuracy')\n",
    "plt.plot(history_1.epoch, history_1.history['val_accuracy'], '-o', label='Validation_accuracy')\n",
    "plt.title('Proposed Model Accuracy')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('reports/Att-Based CNN-BiLSTM_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/Att-Based CNN-BiLSTM_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-framing",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_1.epoch, history_1.history['loss'], '-o', label='Training_loss')\n",
    "plt.plot(history_1.epoch, history_1.history['val_loss'], '-o', label='Validation_loss')\n",
    "plt.title('Proposed model loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.grid(True)\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('reports/Att-Based CNN-BiLSTM_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/Att-Based CNN-BiLSTM_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-mechanism",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = model_1.predict(X_test)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "report = classification_report(y_test, predicted, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-yahoo",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_classification_report(report)\n",
    "plt.savefig('reports/Att-Based CNN-BiLSTM_plot_classif_report_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-employer",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Testing Accuracy: %f' % accuracy)\n",
    "precision = precision_score(y_test, predicted,average='weighted')\n",
    "print('Testing Precision: %f' % precision)\n",
    "recall = recall_score(y_test, predicted,average='weighted')\n",
    "print('Testing Recall: %f' % recall)\n",
    "f1 = f1_score(y_test, predicted,average='weighted')\n",
    "print('Testing F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-latin",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# append the results\n",
    "df_results.loc['Att-Based CNN-BiLSTM'] = [f1, precision, recall, accuracy, train_time]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-resort",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CNN-BiLSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-character",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Y_train=np.array(Y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded, Y_train, test_size=0.1, random_state=42, shuffle= True)\n",
    "\n",
    "# Vectorize the output sentence type classifcations to Keras readable format\n",
    "y_train=to_categorical(y_train, num_classes=2)\n",
    "y_test=to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-flooring",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1650014427972,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "zg9JG59WSWHy",
    "outputId": "92b7957b-6837-4e1a-9cea-2a4d332db8a1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the shape of each set\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-conservation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650014427973,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "a608XJ7QChfq",
    "outputId": "70d5df45-c7f9-4319-c081-6cd8774e9bb4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Train set size = {} \\nTest set size = {}'.format(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-trance",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_2(lr=0.0, lr_d=0.0, spatial_dr=0.0, kernel_size=3,\n",
    "                  dense_units=128, dropout_ratio=0.0, num_filters=32,\n",
    "                  lstm_units_1=0, lstm_units_2=0, batch_size=64, epochs=10):\n",
    "    \n",
    "    model_input = Input(shape=(max_length,))\n",
    "    \n",
    "    ######################## EMBEDDING LAYER ###############################################\n",
    "    # embed MAX_NB_WORDS words into a embed_dim vector. (e.g. For fasttext, the embed__dim is 300)\n",
    "    x = Embedding(\n",
    "        MAX_NB_WORDS, \n",
    "        embed_dim, \n",
    "        input_length = max_length,\n",
    "        weights = [embedding_matrix], \n",
    "        trainable = False,\n",
    "        name = \"embedding\"\n",
    "        )(model_input)\n",
    "\n",
    "    x = SpatialDropout1D(spatial_dr)(x)\n",
    "    \n",
    "    ######################## CNN LAYER(S) ###############################################\n",
    "    ## one kernel\n",
    "    conv = Conv1D(filters=num_filters, kernel_size=kernel_size, padding=\"valid\", activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(conv)  \n",
    "    x = Dropout(dropout_ratio)(x) # yes or no\n",
    "       \n",
    "    ######################## BiLSTM LAYER(S) ################################################\n",
    "    # CuDNNLSTm can be implemented instead of LSTM, it is just implementations of LSTM that are created to run faster on GPUs\n",
    "    x = Bidirectional(LSTM(lstm_units_1, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(lstm_units_2))(x) # yes or no\n",
    "    x = Dense(dense_units, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_ratio)(x)\n",
    "    \n",
    "    ######################## CLASSIFICATION LAYER ###########################################\n",
    "    model_output = Dense(2, activation=\"softmax\")(x)\n",
    "    model = Model(model_input, model_output)\n",
    "    \n",
    "    ######################## COMPILING THE MODEL ##################################\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        loss = \"categorical_crossentropy\", \n",
    "        # optimizer = Adam(learning_rate=lr, decay=lr_d),\n",
    "        optimizer = Adam(learning_rate=lr_schedule),\n",
    "#         optimizer='adam',\n",
    "        metrics = [\n",
    "            \"accuracy\",\n",
    "#             \"binary_accuracy\",\n",
    "#              'categorical_accuracy' ,\n",
    "#             tf.keras.metrics.AUC(),\n",
    "#             tf.keras.metrics.Precision(),\n",
    "#             tf.keras.metrics.Recall()\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    ######################## MODEL SAVING/OVERFITTING MANAGEMENT ##################################\n",
    "    model_path = \"models/best_CNN_BiLSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + \".hdf5\"\n",
    "    check_point = ModelCheckpoint(model_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3, verbose=1,)\n",
    "    \n",
    "    # Display a summary of the models structure\n",
    "    print(model.summary())\n",
    "    print(\"#\"*80)\n",
    "    tf.keras.utils.plot_model(model, to_file=\"reports/cnn_BiLSTM_architecture_\"+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + \".png\", show_shapes=True)\n",
    "    \n",
    "    ######################## TRAINING THE MODEL ##################################\n",
    "    start_time = time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.11111111111111111111111111111,\n",
    "        callbacks=[check_point,early_stop])\n",
    "    train_time = time() - start_time\n",
    "    print(train_time)\n",
    "    model = load_model(model_path, custom_objects=create_custom_objects())\n",
    "    \n",
    "    ######################## EVALUATING THE MODEL ##################################  \n",
    "    model.evaluate(X_test, y_test) # or model.predict\n",
    "    \n",
    "    return model, history, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-appliance",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train proposed model\n",
    "model_2, history_5, train_time = build_model_2(\n",
    "    lr=learning_rate,\n",
    "    # lr_d=learning_decay,\n",
    "    spatial_dr=spa_dropout_ratio,\n",
    "    kernel_size=kernel_size,\n",
    "    dense_units=dense_units,\n",
    "    dropout_ratio=dropout_ratio,\n",
    "    num_filters=num_filters,\n",
    "    lstm_units_1=lstm_units_1,\n",
    "    lstm_units_2=lstm_units_2,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363bba8e9625158",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-court",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_5.epoch, history_5.history['accuracy'], '-o', label='training_accuracy')\n",
    "plt.plot(history_5.epoch, history_5.history['val_accuracy'], '-o', label='validation_accuracy')\n",
    "plt.title('CNN-BiLSTM Model Accuracy')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('reports/CNN-BiLSTM_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/CNN-BiLSTM_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-horizon",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_5.epoch, history_5.history['loss'], '-o', label='Training_loss')\n",
    "plt.plot(history_5.epoch, history_5.history['val_loss'], '-o', label='Validation_loss')\n",
    "plt.title('CNN-BiLSTM Model loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.grid(True)\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('reports/CNN-BiLSTM_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/CNN-BiLSTM_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-costs",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = model_2.predict(X_test)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "report = classification_report(y_test, predicted, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-throat",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_classification_report(report)\n",
    "plt.savefig('reports/CNN-BiLSTM_plot_classif_report_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-chemical",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Testing Accuracy: %f' % accuracy)\n",
    "\n",
    "precision = precision_score(y_test, predicted,average='weighted')\n",
    "print('Testing Precision: %f' % precision)\n",
    "\n",
    "recall = recall_score(y_test, predicted,average='weighted')\n",
    "print('Testing Recall: %f' % recall)\n",
    "\n",
    "f1 = f1_score(y_test, predicted,average='weighted')\n",
    "print('Testing F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-floating",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# append the results\n",
    "df_results.loc['CNN-BiLSTM'] = [f1, precision, recall, accuracy, train_time]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-touch",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BASELINES MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-oxygen",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1D CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-graphic",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Y_train=np.array(Y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded, Y_train, test_size=0.1, random_state=42, shuffle= True)\n",
    "\n",
    "# Vectorize the output sentence type classifcations to Keras readable format\n",
    "y_train=to_categorical(y_train, num_classes=2)\n",
    "y_test=to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-scanning",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1650014427972,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "zg9JG59WSWHy",
    "outputId": "92b7957b-6837-4e1a-9cea-2a4d332db8a1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the shape of each set\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-bennett",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650014427973,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "a608XJ7QChfq",
    "outputId": "70d5df45-c7f9-4319-c081-6cd8774e9bb4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Train set size = {} \\nTest set size = {}'.format(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-newsletter",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, embed_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "\n",
    "model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding=\"valid\", activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(dense_units, activation='relu'))\n",
    "model.add(Flatten()) # no need if used Global poooling\n",
    "model.add(Dropout(dropout_ratio))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "######################## COMPILING THE MODEL ##################################\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    # optimizer = Adam(learning_rate=learning_rate, decay=learning_decay),\n",
    "    optimizer = Adam(learning_rate=lr_schedule),\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "#       \"binary_accuracy\",\n",
    "        'categorical_accuracy',\n",
    "#       tf.keras.metrics.AUC(),\n",
    "#       tf.keras.metrics.Precision(),\n",
    "#       tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-survey",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/1D_CNN_\"+ datetime.now().strftime(\"%Y%m%d_%H:%M:%S\") + \".h5\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor = 'loss',\n",
    "    verbose = 1, \n",
    "    patience = 3, # Number of epochs with no improvement after which training will be stopped.\n",
    "    mode = 'min'\n",
    ")\n",
    "callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "# fit network\n",
    "start_time = time()\n",
    "history_2 = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,callbacks=callbacks_list,validation_split=0.11111111111111111111111111111)\n",
    "train_time = time() - start_time\n",
    "print(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-twelve",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "report = classification_report(y_test, predicted, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9999340ba5c89",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Plotting the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-access",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_2.epoch, history_2.history['accuracy'], '-o', label='Training_accuracy')\n",
    "plt.plot(history_2.epoch, history_2.history['val_accuracy'], '-o', label='Validation_accuracy')\n",
    "plt.title('1D CNN model accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('reports/1D_CNN__accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H:%M:%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/1D_CNN__accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H:%M:%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-direction",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_2.epoch, history_2.history['loss'], '-o', label='training_loss')\n",
    "plt.plot(history_2.epoch, history_2.history['val_loss'], '-o', label='validation_loss')\n",
    "plt.legend()\n",
    "plt.title('1D CNN model loss')\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.grid(True)\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('reports/1D_CNN__loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H:%M:%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/1D_CNN__loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H:%M:%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-spoke",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_classification_report(report)\n",
    "plt.savefig('reports/1D_CNN__plot_classif_report_'+ datetime.now().strftime(\"%Y%m%d_%H:%M:%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-edition",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Testing Accuracy: %f' % accuracy)\n",
    "precision = precision_score(y_test, predicted,average='weighted')\n",
    "print('Testing Precision: %f' % precision)\n",
    "recall = recall_score(y_test, predicted,average='weighted')\n",
    "print('Testing Recall: %f' % recall)\n",
    "f1 = f1_score(y_test, predicted,average='weighted')\n",
    "print('Testing F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-method",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# append the results\n",
    "df_results.loc['1D_CNN'] = [f1, precision, recall, accuracy, train_time]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-artist",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-divorce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Y_train=np.array(Y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded, Y_train, test_size=0.1, random_state=42, shuffle= True)\n",
    "\n",
    "# Vectorize the output sentence type classifcations to Keras readable format\n",
    "y_train=to_categorical(y_train, num_classes=2)\n",
    "y_test=to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-woman",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1650014427972,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "zg9JG59WSWHy",
    "outputId": "92b7957b-6837-4e1a-9cea-2a4d332db8a1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the shape of each set\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-dispute",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650014427973,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "a608XJ7QChfq",
    "outputId": "70d5df45-c7f9-4319-c081-6cd8774e9bb4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Train set size = {} \\nTest set size = {}'.format(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-genealogy",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, embed_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "model.add(LSTM(lstm_units_1,return_sequences=True))\n",
    "model.add(LSTM(lstm_units_2))\n",
    "model.add(Dense(dense_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "######################## COMPILING THE MODEL ##################################\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    # optimizer = Adam(learning_rate=learning_rate, decay=learning_decay),\n",
    "    optimizer = Adam(learning_rate=lr_schedule),\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "#             \"binary_accuracy\",\n",
    "             'categorical_accuracy',\n",
    "#             tf.keras.metrics.AUC(),\n",
    "#         tf.keras.metrics.Precision(),\n",
    "#         tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-shoot",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/LSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + \".h5\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor = 'loss',\n",
    "    verbose = 1, \n",
    "    patience = 3, # Number of epochs with no improvement after which training will be stopped.\n",
    "    mode = 'min'\n",
    ")\n",
    "callbacks_list = [checkpoint,earlystopping]\n",
    "\n",
    "# fit network\n",
    "start_time = time()\n",
    "history_3 = model.fit(X_train, y_train,batch_size=batch_size, epochs=epochs, verbose=1,callbacks=callbacks_list,validation_split=0.11111111111111111111111111111)\n",
    "train_time = time() - start_time\n",
    "print(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-proceeding",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "report = classification_report(y_test, predicted, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c203f9e44a95a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Plotting the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-monaco",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_3.epoch, history_3.history['accuracy'], '-o', label='Training_accuracy')\n",
    "plt.plot(history_3.epoch, history_3.history['val_accuracy'], '-o', label='Validation_accuracy')\n",
    "plt.title('LSTM model accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('reports/LSTM_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/LSTM_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-ceiling",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_3.epoch, history_3.history['loss'], '-o', label='Training_loss')\n",
    "plt.plot(history_3.epoch, history_3.history['val_loss'], '-o', label='Validation_loss')\n",
    "plt.legend()\n",
    "plt.title('LSTM model loss')\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.grid(True)\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('reports/LSTM_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/LSTM_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-blond",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_classification_report(report)\n",
    "plt.savefig('reports/LSTM_plot_classif_report_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-mother",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Testing Accuracy: %f' % accuracy)\n",
    "precision = precision_score(y_test, predicted,average='weighted')\n",
    "print('Testing Precision: %f' % precision)\n",
    "recall = recall_score(y_test, predicted,average='weighted')\n",
    "print('Testing Recall: %f' % recall)\n",
    "f1 = f1_score(y_test, predicted,average='weighted')\n",
    "print('Testing F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-coordination",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# append the results\n",
    "df_results.loc['LSTM'] = [f1, precision, recall, accuracy, train_time]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-value",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BiLSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-royalty",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Y_train=np.array(Y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded, Y_train, test_size=0.1, random_state=42, shuffle= True)\n",
    "\n",
    "# Vectorize the output sentence type classifcations to Keras readable format\n",
    "y_train=to_categorical(y_train, num_classes=2)\n",
    "y_test=to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-ceramic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1650014427972,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "zg9JG59WSWHy",
    "outputId": "92b7957b-6837-4e1a-9cea-2a4d332db8a1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the shape of each set\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-incidence",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1650014427973,
     "user": {
      "displayName": "Milandu Keith Moussavou Boussougou",
      "userId": "16125998280788005643"
     },
     "user_tz": -540
    },
    "id": "a608XJ7QChfq",
    "outputId": "70d5df45-c7f9-4319-c081-6cd8774e9bb4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Train set size = {} \\nTest set size = {}'.format(len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-official",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, embed_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(SpatialDropout1D(spa_dropout_ratio))\n",
    "model.add(Bidirectional(LSTM(lstm_units_1,return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(lstm_units_2)))\n",
    "model.add(Dense(dense_units, activation='relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "######################## COMPILING THE MODEL ##################################\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = Adam(learning_rate=lr_schedule),\n",
    "    # optimizer = Adam(learning_rate=learning_rate, decay=learning_decay),\n",
    "#         optimizer='adam',\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "#             \"binary_accuracy\",\n",
    "             'categorical_accuracy',\n",
    "#             tf.keras.metrics.AUC(),\n",
    "#         tf.keras.metrics.Precision(),\n",
    "#         tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-score",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/BiLSTM_\"+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + \".h5\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor = 'loss',\n",
    "    verbose = 1, \n",
    "    patience = 3, # Number of epochs with no improvement after which training will be stopped.\n",
    "    mode = 'min'\n",
    ")\n",
    "callbacks_list = [checkpoint,earlystopping]\n",
    "\n",
    "# fit network\n",
    "start_time = time()\n",
    "history_4 = model.fit(X_train, y_train,batch_size=batch_size, epochs=epochs, verbose=1,callbacks=callbacks_list,validation_split=0.11111111111111111111111111111)\n",
    "train_time = time() - start_time\n",
    "print(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-marking",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "report = classification_report(y_test, predicted, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a24624ad69ed81",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### **Plotting the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-bennett",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_4.epoch, history_4.history['accuracy'], '-o', label='Training_accuracy')\n",
    "plt.plot(history_4.epoch, history_4.history['val_accuracy'], '-o', label='Validation_accuracy')\n",
    "plt.title('BiLSTM model accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('reports/BiLSTM_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/BiLSTM_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-validity",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_4.epoch, history_4.history['loss'], '-o', label='Training_loss')\n",
    "plt.plot(history_4.epoch, history_4.history['val_loss'], '-o', label='Validation_loss')\n",
    "plt.legend()\n",
    "plt.title('BiLSTM model loss')\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.grid(True)\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('reports/BiLSTM_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/BiLSTM_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-spoke",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_classification_report(report)\n",
    "plt.savefig('reports/BiLSTM_plot_classif_report_'+ datetime.now().strftime(\"%Y%m%d_%H-%M%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-latest",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print('Testing Accuracy: %f' % accuracy)\n",
    "precision = precision_score(y_test, predicted,average='weighted')\n",
    "print('Testing Precision: %f' % precision)\n",
    "recall = recall_score(y_test, predicted,average='weighted')\n",
    "print('Testing Recall: %f' % recall)\n",
    "f1 = f1_score(y_test, predicted,average='weighted')\n",
    "print('Testing F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-prevention",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# append the results\n",
    "df_results.loc['BiLSTM'] = [f1, precision, recall, accuracy, train_time]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-neighborhood",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## save the results in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-nancy",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save the training results\n",
    "df_results.to_csv(\"models/Models_performance_summary_\" + datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-mistake",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot overall accuracy on test set\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "plt.plot(history_2.epoch, history_2.history['val_accuracy'], '-o', label='1D CNN')\n",
    "plt.plot(history_3.epoch, history_3.history['val_accuracy'], '-o', label='LSTM')\n",
    "plt.plot(history_4.epoch, history_4.history['val_accuracy'], '-o', label='BiLSTM')\n",
    "plt.plot(history_5.epoch, history_5.history['val_accuracy'], '-o', label='CNN-BiLSTM')\n",
    "plt.plot(history_1.epoch, history_1.history['val_accuracy'], '-o', label='Proposed')\n",
    "plt.title('Validation Accuracy of All Models')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('reports/All_models_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/All_models_accuracy_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-blake",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot overall looss on test set\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "plt.plot(history_2.epoch, history_2.history['val_loss'], '-o', label='1D CNN')\n",
    "plt.plot(history_3.epoch, history_3.history['val_loss'], '-o', label='LSTM')\n",
    "plt.plot(history_4.epoch, history_4.history['val_loss'], '-o', label='BiLSTM')\n",
    "plt.plot(history_5.epoch, history_5.history['val_loss'], '-o', label='CNN-BiLSTM')\n",
    "plt.plot(history_1.epoch, history_1.history['val_loss'], '-o', label='Proposed')\n",
    "plt.title('Validation Loss of All Models')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlim(left=0)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('reports/All_models_val_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.png', dpi=600, format='png', bbox_inches='tight')\n",
    "plt.savefig('reports/All_models_val_loss_metrics_'+ datetime.now().strftime(\"%Y%m%d_%H-%M-%S\") + '.pdf', dpi=600, format='pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb10e6ddb6cbec6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
